{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sergio-Rodriguez24/Prediction-of-Product-Sales/blob/main/Preprocessing_Code_along.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps\n",
        "1. Import all NECESSARY libraries\n",
        "2. Load the dataset\n",
        "3. Make copy of dataframe (in case you make a mistake)\n",
        "4. Check `.info()` for datatypes and missing values\n",
        "5. Check for duplicates - `df.duplicated().sum()`\n",
        "  \n",
        "  a. Drop duplicates if necessary `df.drop_duplicates(inplace=True)`\n",
        "6. Check for impossible outliers in numeric data - `df.describe()`\n",
        "\n",
        "  a. Drop or fix impossible outlisers if necessary - `df.describe(include='number')`\n",
        "\n",
        "7. Check for inconsistencies in categorical data - `df.value_counts()`\n",
        "\n",
        "  a. Change category names if necessary - `df.replace(replacement_dict, inplace=True)`\n",
        "8. Split the dataset- (X, y) and (train, test)\n",
        "9. preprocess the data\n",
        "10. Explain and justify the strategy used in code like why are you using mean, median or mode?\n",
        "11. Instantiate Transformers and selectors\n",
        "12. Make pipelines (if needed) - `make_pipeline(trans1, trans2)`\n",
        "13. Make tuples - `(pipeline, selector)`\n",
        "10. Put pipelines into column transformer. `make_column_selector(tuple1, tuple2)`\n",
        "11. Fit the column transformer to **train only**, and transform train AND test.  \n"
      ],
      "metadata": {
        "id": "0uExw6mPoDQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G4eBaneNqSG"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task:\n",
        "Predict the sale price of a diamond based on measurements of the diamond\n",
        "\n",
        "carat\n",
        "weight of the diamond\n",
        "\n",
        "cut\n",
        "quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
        "\n",
        "color\n",
        "diamond colour, from J (worst) to D (best)\n",
        "\n",
        "clarity\n",
        "a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
        "\n",
        "x\n",
        "length in mm\n",
        "\n",
        "y\n",
        "width in mm\n",
        "\n",
        "z\n",
        "depth in mm\n",
        "\n",
        "table\n",
        "\n",
        "width of top of diamond relative to widest point"
      ],
      "metadata": {
        "id": "BbV__sfX5w4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps:\n",
        "\n",
        "1. Load the Data\n",
        "2. Check for duplicates, missing values, errors\n",
        "3. Split the data (X, y and train, test)\n",
        "4. Instantiate the Transformers and column selectors\n",
        "5. Create numeric preprocessing pipeline and categorical preprocessing pipeline\n",
        "6. Combine pipelines using column transformer\n",
        "7. Fit preprocessor on X_train\n",
        "8. Transform both X_train and X_test\n",
        "9. (Optional) convert processed X_train back to a dataframe."
      ],
      "metadata": {
        "id": "7YWXYUr7_Sgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "nYr3ZlaS_Q5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTpBcQrzjkPeQ4W_sZuOa45qqxLBgtr_wzistIB3a7gFUJ1AlBhloV5g9IxV5DFnd-GPktXDNprGOUh/pub?gid=88799590&single=true&output=csv')"
      ],
      "metadata": {
        "id": "eTRaEHFbN8sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YeyK1EuMODEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make a copy of original df to avoid any manipulations"
      ],
      "metadata": {
        "id": "dqS1isRUDBac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of original df to avoid any manipulations\n",
        "eda_ml = df.copy()"
      ],
      "metadata": {
        "id": "qlAE3K35DAW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unneccessary columns\n",
        "eda_ml = eda_ml.drop('Unnamed: 0', axis=1)"
      ],
      "metadata": {
        "id": "zJx_eZ7aAvrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the dataframe\n",
        "eda_ml.head()"
      ],
      "metadata": {
        "id": "VtPX64qY0GwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for Duplicated, Missing, or Erroneous Data"
      ],
      "metadata": {
        "id": "T2ty6Pq5AXLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see if there are any duplicate rows\n",
        "eda_ml.duplicated().sum()"
      ],
      "metadata": {
        "id": "lA8kag6aOFrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicte rows\n",
        "eda_ml.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "bnQ_SW57Gc1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm now duplicate rows remain\n",
        "eda_ml.duplicated().sum()"
      ],
      "metadata": {
        "id": "J98mzBRNuQel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary info\n",
        "eda_ml.info()"
      ],
      "metadata": {
        "id": "GQcJ8AYvkIsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the sum of missing values\n",
        "eda_ml.isna().sum().sum()"
      ],
      "metadata": {
        "id": "VNdgEu-vA9yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the sum of missing values\n",
        "eda_ml.isna().sum()"
      ],
      "metadata": {
        "id": "_11g6cIE0pDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display desriptive statitistics for all collumns\n",
        "eda_ml.describe(include='number')"
      ],
      "metadata": {
        "id": "k2rdj9xW-kld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display desriptive statitistics for all collumns\n",
        "eda_ml.describe(include='object')"
      ],
      "metadata": {
        "id": "KoeSLYMdHuiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Values: Drop Rows or Impute?"
      ],
      "metadata": {
        "id": "ZcVD12J3GMrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check target for null values\n",
        "eda_ml['price'].isna().sum()"
      ],
      "metadata": {
        "id": "Gku6Ci4QM0wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows without a Target Value\n",
        "eda_ml.dropna(subset=['price'], inplace=True)"
      ],
      "metadata": {
        "id": "jmOk-yc9GQi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many rows would we lose if we just dropped the rows with missing values?"
      ],
      "metadata": {
        "id": "J-jDZi7yITiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percent of total rows missing values\n",
        "percent_missing = (1 - eda_ml.dropna().shape[0] / df.shape[0]) * 100\n",
        "print(f'{percent_missing:.4f} percent of rows are missing at least 1 value')"
      ],
      "metadata": {
        "id": "XZfkNpe0IJA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ordinal Encode 'cut' and 'clarity'"
      ],
      "metadata": {
        "id": "WDrzPXHiCVGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `cut` column was pretty clear in how to order them, but the clarity column took some research.  [Selecting a Diamond](https://selectingadiamond.com/diamond-clarity/)\n",
        "\n",
        "![Diamond Clarity Chart](https://selectingadiamond.com/wp-content/uploads/2019/10/Diamond-Clarity.jpg)"
      ],
      "metadata": {
        "id": "UaSsvUD8KRDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ordinal categories\n",
        "eda_ml['cut'].value_counts()\n"
      ],
      "metadata": {
        "id": "s5fBZAQF1EuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda_ml['clarity'].value_counts()"
      ],
      "metadata": {
        "id": "Vt_Xnu_g3A3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda_ml['cut'].unique()"
      ],
      "metadata": {
        "id": "sLPdKI_1BI23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the Data (Validation Split)"
      ],
      "metadata": {
        "id": "G2eS-1HCBCSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split X and y, we are predicting price\n",
        "target = 'price'\n",
        "X = eda_ml.drop(columns=[target]).copy()\n",
        "y = eda_ml[target].copy()\n",
        "\n",
        "# split training and test\n",
        "\n",
        "# set random_state to 42 for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30 , random_state=42)"
      ],
      "metadata": {
        "id": "g5XFIN3tX6PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "tXneQU6eB1kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "8otQM37iB5yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.dtypes"
      ],
      "metadata": {
        "id": "SbBajfDl346f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Pipelines and Tuples for Each Group of Columns\n",
        "\n",
        "I'm going to divide my data into\n",
        "\n",
        "- numeric\n",
        "- nominal categorical, and\n",
        "- ordinal categorical columns\n",
        "\n",
        "and preprocess each subset differently.\n"
      ],
      "metadata": {
        "id": "zwjEXLMsBF9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Numeric"
      ],
      "metadata": {
        "id": "w8p46b5JiVWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING PIPELINE FOR NUMERIC DATA\n",
        "\n",
        "# Save list of number column names\n",
        "\n",
        "\n",
        "# Transformers\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "2Jdx3KqUgLLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuple\n"
      ],
      "metadata": {
        "id": "FzeAT0qzPpuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Ordinal"
      ],
      "metadata": {
        "id": "z-q8OQCcg1a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING PIPELINE FOR ORDINAL DATA\n",
        "\n",
        "# Save list of number column names\n",
        "\n",
        "\n",
        "# Ordered Category Lists\n",
        "\n",
        "\n",
        "# Transformers\n",
        "\n",
        "\n",
        "\n",
        "# you might have 100 diff cat for ordinal so its getting out of range so good to scale\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "\n",
        "\n",
        "# Tuple\n",
        "\n"
      ],
      "metadata": {
        "id": "PswigsfNg3u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Nominal"
      ],
      "metadata": {
        "id": "HrZC0sgkgcXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING PIPELINE FOR ONE-HOT-ENCODED DATA\n",
        "\n",
        "# Save list of nominal column names\n",
        "\n",
        "\n",
        "# Transformers\n",
        "\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "\n",
        "\n",
        "# Tuple\n"
      ],
      "metadata": {
        "id": "CPEVXXdhgfHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_cols"
      ],
      "metadata": {
        "id": "yd_i4GkuQt35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Column Transformer to Apply Different Preprocessing to Different Columns"
      ],
      "metadata": {
        "id": "PbHNwNUhBhQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the make column transformer\n",
        "col_transformer = ColumnTransformer([numeric_tuple,\n",
        "                                       ord_tuple,\n",
        "                                       ohe_tuple],\n",
        "                                       remainder='drop', verbose_feature_names_out=False)\n"
      ],
      "metadata": {
        "id": "NppL3ZSGi58_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Column Transformer on the Training Data Only"
      ],
      "metadata": {
        "id": "9qUwkuT9Bo9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the column transformer on the X_train\n",
        "col_transformer.fit(X_train)"
      ],
      "metadata": {
        "id": "XEhzAUVuZQtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform Both Training and Testing Data"
      ],
      "metadata": {
        "id": "kJLeX9eUBvsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the default transformation output to Pandas\n",
        "from sklearn import set_config\n",
        "set_config(transform_output='pandas')"
      ],
      "metadata": {
        "id": "0xnfAwRsGECm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the X_train and the X_test\n",
        "X_train_proc = col_transformer.transform(X_train)\n",
        "X_test_proc = col_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "wdSRCD9eGEDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZJgeTZmfSfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the Result"
      ],
      "metadata": {
        "id": "VAlAXVS7B0I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first (5) rows of the dataframe\n",
        "display(X_train_proc.head())\n",
        "# Check the shape\n",
        "print(f'\\nshape of processed data is: {X_train_proc.shape}')\n",
        "# Check for remaining missing values\n",
        "print(f'\\nThere are {X_train_proc.isna().sum().sum()} missing values')\n",
        "# Check the data types\n",
        "print(f'\\nThe datatypes are {X_train_proc.dtypes}')"
      ],
      "metadata": {
        "id": "2ZmMQiFZGY3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Bonus!) Preview of Next Week!  Modeling!"
      ],
      "metadata": {
        "id": "xv2LxXUs6f7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import model and metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "VganKnvGFiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "lr = LinearRegression()\n",
        "# create model pipeline\n",
        "lr_pipe = make_pipeline(col_transformer, lr)\n",
        "# fit model pipeline\n",
        "# this fits ALL transformers AND the model\n",
        "lr_pipe.fit(X_train, y_train)\n",
        "# make predictions with BOTH the training and testing set\n",
        "\n",
        "train_predictions = lr_pipe.predict(X_train)\n",
        "test_predictions = lr_pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "2d6bfnVWFmum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions[:5]"
      ],
      "metadata": {
        "id": "gr5xTrsXU27a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "Iq0kG_IuU6AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the average error of the model for train and test sets\n",
        "train_error = mean_absolute_error(y_train, train_predictions)\n",
        "test_error = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "print(f'The models average error on the training set is {train_error}')\n",
        "print(f'the models average error on the testing set is {test_error}')"
      ],
      "metadata": {
        "id": "IFqCsBX_F9hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fg0gIV_xMDUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eB_jQZBBU2XW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}